---
title: "Predicting the Number of Unemployment Insurance Claims in the Time of COVID-19"
author: "submitted by Radhika Kaul, Odiche Nwabuikwu, & Ruochen Wang"
output:
  html_document:
    df_print: paged
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

## Question of Interest

We will develop different models to predict the number of unemployment claims filed in each US state in the second week of May (when the project is due). Our primary data source for the outcome variable is the [Unemployment Insurance Weekly Claims data](https://oui.doleta.gov/unemploy/claims.asp) from the U.S. Department of Labor (DOL)'s Employment & Training Administration.

## Project Progress

We have decided on the following variables for our models:

**Outcome Variables**: either New Initial Claims filed per week OR percentage of the labor force filing for unemployment claims per week. New Initial Claims is one of the most sensitive and most frequently used official statistics in analyzing unemployment trends (see [Aaron Soujourner and Paul Goldsmith Pinkham](https://paulgp.github.io/GoogleTrendsUINowcast/google_trends_UI.html)).

**Predictors** *(all variables are at the state level)*

Structural predictors include:

- Region
- Population
- Working Population
- Per Capita Personal Income
- Percent Population w/ a High School Degree
- Top Industry in terms of GDP Contribution
- Democratic or Republican Governor

We are considering standardizing/normalizing/logging the population variables due to concerns of high correlations between these variables and the outcome variable.

Real-Time Predictors include:

- Number of COVID Cases
- Number of Days Since the Issurance of Stay-at-Home Order
- Number of Days Before Stay-at-Home Order Ends
- Peak of Claims Predicted *(we haven't found a data source yet but will keep looking)*
- Whether there has been a protest/protests in the state

**Predictive Models**: We will have three predictive models based off of data from when the first case was reported in the US, the first death was reported in the US, and the modified date when the first death related to COVID-19 was reported (see https://www.cnn.com/2020/04/22/us/california-deaths-earliest-in-us/index.html)

**Algorithms**: KNN/CART/Random Forest. We will test each algorithm and get the error rates and pick the best one.

We propose a **Random Forest** model given that we have both categorical and continuous inputs and that we have correlated predictors. Also given the fluctuation in the weekly claims data, Random Forest will make more accurate out-of-sample predictions as compared to KNN and CART.

**Error Metrics**: RMSE (OOB Error for Random Forest)

## Data Cleaning

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages, echo=T, message=F, warning=F}

library(readxl)
library(haven)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(treemapify)
```

### Unemployment Claims

```{r unemp-clean, echo=T, message=F, warning=F}

claims <- read_excel("data/raw/unemployment_bystate_weekly.xls")

claims <- claims %>%
  rename(state_name = State) %>%
  rename_all(~str_to_lower(colnames(claims))) %>%
  rename_all(~str_replace_all(., "\\s+", "_")) %>%
  filter(state != "Puerto Rico" &
           state != "Virgin Islands" ) %>%
  select(state, filed_week_ended, initial_claims, continued_claims)

# change data to wide
claims <-
  pivot_wider(data = claims,
              names_from = filed_week_ended,
              values_from = c(initial_claims, continued_claims))
```

### GDP by Industry

```{r gdp-clean, echo=T, message=F, warning=F}

gdp <-
  read.csv("data/raw/gdp_bystate_byindustry_2019.csv")

gdp <- gdp %>%
  filter(str_detect(string = Description,
                    pattern = "    ")) %>% 
  filter(!str_detect(string = Description, 
                     pattern = "     ")) %>% # remove aggregate & sub-industries
  filter(GeoName != "United States") %>% # remove federal data
  filter(X2019 != "(NA)" & X2019 != "(D)") %>% # remove non-numeric values
  rename(gdp2019 = X2019, state = GeoName) # rename GDP and state variables

# change data to wide
gdp <- gdp %>% 
  pivot_wider(names_from = Description, state,
              values_from = gdp2019) %>%
  filter(!state %in% c("New England", "Mideast", "Great Lakes", "Plains", "Southeast", "Southwest", "Rocky Mountain", "Far West"))

# find top industry wrt gdp
gdp <- gdp %>%
  mutate(topind_gdp = colnames(gdp)[apply(gdp, 1, which.max)]) %>%
  select(state, topind_gdp)
```

### Per Capita Personal Income

```{r inc-clean, echo=T, message=F, warning=F}

income <- read_csv("data/raw/percapitapersonalincome_bystate_2019.csv")

income <- income %>%
  filter(as.numeric(GeoFips) >= 1000 & as.numeric(GeoFips) <= 56000) %>% # remove fed/regional data
  rename(personal_inc2019 = "2019", state = GeoName) %>%
  select(-GeoFips)
```

### Employment by Occupation

```{r occ-clean, echo=T, message=F, warning=F}

occ <- read_excel("data/raw/employment_byoccupation_bystate_may2019.xlsx")

occ <- occ %>%
  filter(o_group == "major") %>%
  select(area_title, occ_code, occ_title, jobs_1000) %>%
  mutate(jobs_1000 = as.numeric(jobs_1000)) %>%
  rename(state = area_title) %>%
  group_by(state) %>%
  mutate(rank = rank(-jobs_1000)) %>%
  filter(rank <= 3) %>%
  select(-jobs_1000)

# change data to wide and rename vars
occ <- occ %>% 
  pivot_wider(names_from = rank, state,
              values_from = occ_title) %>%
  rename(topindemp_3 = 2, topindemp_1 = 3, topindemp_2 = 4)
```

ATT: need further discussion since not much variance

### Census Region

```{r regions-clean, echo=T, message=F, warning=F}

regions <- read_excel("data/raw/censusregion_bystate.xlsx")

regions <- regions %>%
  select(State, Region) %>%
  rename(state = State, region = Region)
```

### Merge Datasets

```{r merge, echo=T, message=F, warning=F}
clean_data <-
  left_join(claims, income, by = "state")
anti_join(clean_data, income)

clean_data <-
  left_join(clean_data, gdp, by = "state")
anti_join(clean_data, gdp)

clean_data <-
  left_join(clean_data, occ, by = "state")
anti_join(clean_data, occ)

clean_data <-
  left_join(clean_data, regions, by = "state")
anti_join(clean_data, regions)
```

```{r merge-sah-date, echo=T, message=F, warning=F}

# load & add stay at home date data
sah <- read_excel("data/raw/stayathome.xlsx")

clean_data <-
  left_join(clean_data, sah, by = "state")
anti_join(clean_data, sah)
```

```{r merge-peak-date,  echo=T, message=F, warning=F}

#load & add peak death data

sah_peakdeaths <- read_excel("data/raw/sah_peakdeaths.xlsx")

clean_data <-
  left_join(clean_data, sah_peakdeaths, by = "state") %>%
  rename(massgath_res_date = mg_restr,
         noness_res_date = noness_restr)
anti_join(clean_data, sah_peakdeaths)
```

```{r merge-ed-attainment,  echo=T, message=F, warning=F}

# load & add ed attainment data
edattain <- read_csv("data/raw/edattain_bystate_2018.csv")

clean_data <-
  left_join(clean_data, edattain, by = "state")
anti_join(clean_data, edattain)
```

```{r merge-working-pop,  echo=T, message=F, warning=F}

# load & add working pop data
pctworkpop <- read_csv("data/raw/popest_bystate_2018.csv") %>%
  select(state, totpop_2018, totpop_work, pctpop_work)

clean_data <-
  left_join(clean_data, pctworkpop, by = "state")
anti_join(clean_data, pctworkpop)
```

```{r merge-gov-party,  echo=T, message=F, warning=F}

# load & add governor party data
party <- read_excel("data/raw/party_bystate_2020.xlsx")

clean_data <-
  left_join(clean_data, party, by = "state")
anti_join(clean_data, party)
```

```{r merge-death-num-till-apr10, echo=T, message=F, warning=F}

# load packages
library(httr)
library(RCurl)

# read in JHU COVID data from Github for April 18, 2020
jhufile_1 = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/04-18-2020.csv"

mydata <- read.csv(url(jhufile_1))

jhu_data <- mydata %>%
  rename(state = "Province_State") %>%
  drop_na(FIPS) %>%
  filter(Country_Region == "US") 

jhu_data <- jhu_data[!(is.na(jhu_data$Admin2) | jhu_data$Admin2==""),] %>%
  select(state, Last_Update, Confirmed, Deaths, Recovered)

jhu_data_final <- jhu_data %>% 
  group_by(state) %>%
  summarise(sum_deaths = sum(Deaths),
            sum_conf = sum(Confirmed))

clean_data <-
  left_join(clean_data, jhu_data_final, by = "state")
anti_join(clean_data, jhu_data_final)
```

```{r merge-incidence-and-mortality-rate-apr12, echo=T, message=F, warning=F}

# read in JHU data on incident and mortality rates from Github for April 18, 2020
jhufile_2 = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/04-18-2020.csv"

rates_data <- read.csv(url(jhufile_2))

jhu_data_1 <- rates_data %>%
  filter(Country_Region == "US") %>%
  rename(state = "Province_State") %>%
  select(FIPS, state, Confirmed, Deaths, Lat, Long_, Incident_Rate, Mortality_Rate) %>%
  filter(FIPS < 60)

clean_data <-
  left_join(clean_data, jhu_data_1, by = "state")
anti_join(clean_data, jhu_data_1)
```

### Create Final Analytical Data

```{r final-merge, echo=T, message=F, warning=F}

clean_data %>% write_csv("data/clean/merged.csv")
```

## Descriptive Analysis

```{r make-line-graph, echo=T, message=F, warning=F}

# build line graph data
line_data <- clean_data %>%
  select(state, 'initial_claims_2020-01-25':'initial_claims_2020-04-18', Incident_Rate) %>%
  mutate(rank = rank(-Incident_Rate)) %>%
  filter(rank <=5) %>%
  gather(key = week_ended, value = initialclaims, 2:14) %>%
  mutate(week_ended = sub("initial_claims_", "", week_ended)) %>%
  mutate(week_ended = as.Date(week_ended))

# make line graph
state_rank <- c("New York",
                "New Jersey",
                "Massachusetts",
                "Louisiana",
                "Connecticut")
line_data$state <- factor(line_data$state, state_rank)

ggplot(data = line_data,
       mapping = aes(x = week_ended,
                     y = initialclaims,
                     group = state,
                     color = state)) +
  scale_x_date(date_breaks = "1 week") +
  geom_line() +
  geom_vline(xintercept = as.Date("2020-03-19"), linetype = 2) +
  theme_classic() +
  labs(title = "Weekly Unemployment Insurance Claims in Top 5 States\nwith Highest COVID-19 Incidence Rate",
       caption = "Source: US Department of Labor & Johns Hopkins University",
       x = "Week Ending",
       y = "Number of Initial Claims") +
  theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 4),
        plot.caption = element_text(size = 7, family = "Arial", face = "italic", vjust = -4),
        axis.title.x = element_text(family = "Arial", vjust = -2),
        axis.title.y = element_text(family = "Arial", vjust = .5),
        axis.text.x = element_text(angle = 45, size = 7, family = "Arial", hjust = 1),
        axis.text.y = element_text(family = "Arial"),
        axis.ticks.x = element_blank())
```

```{r make-treemap, echo=T, message=F, warning=F}

# build treemap data
treemap_data <- clean_data %>%
  select(state, region, `initial_claims_2020-04-18`, `continued_claims_2020-04-18`, Incident_Rate) %>%
  mutate(`totclaims_2020_04_18` = `continued_claims_2020-04-18` + `initial_claims_2020-04-18`) %>%
  select(state, region, `totclaims_2020_04_18`, Incident_Rate) %>%
  write_csv("treemap.csv")

# make treemap
ggplot(data = treemap_data,
       mapping = aes(area = `totclaims_2020_04_18`,
                     fill = Incident_Rate,
                     label = `state`,
                     subgroup = region)) +
  geom_treemap() +
  geom_treemap_subgroup_border(color = "white") +
  geom_treemap_text(color = "white",
                    place = "center",
                    grow = F,
                    reflow = T) +
  geom_treemap_subgroup_text(color = "#FAFAFA",
                             place = "center",
                             grow = T,
                             alpha = .5,
                             min.size = 0) +
  scale_fill_gradient(low = "#fdbb84", high = "#b30000",
                      breaks = seq(50, 1500, by = 500),
                      name = "Number of COVID Cases\nper 100,000 Residents") +
  labs(title = "Total Unemployment Insurance Claims Filed, as of April 18",
       caption = "Source: US Department of Labor & Johns Hopkins University") +
  theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 2),
        plot.caption = element_text(size = 7, family = "Arial", face = "italic", vjust = -2),
        legend.title = element_text(size = 9, family = "Arial", vjust = 4),
        legend.text = element_text(family = "Arial"))
```

