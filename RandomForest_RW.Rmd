---
title: "Random Forest_RW"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages, echo=T, message=F, warning=F}

library(haven)
library(tidyverse)
library(tidymodels)
library(ranger)
```

```{r split-and-preprocess-data, echo=T, message=F, warning=F}

# read in data
claims_data <- read_csv("~/Downloads/DataScienceAssignments/groupproject/data/clean/merged_test.csv")

# split data into training and testing sets
set.seed(20200504)

claims_split <- initial_split(claims_data, prop = .8)
claims_training <- training(claims_split)
claims_testing <- testing(claims_split)
#and the testing set is now under lock and key!!!

# create recipe
recipe_claims <- recipe(formula = initial_claims_2020_04_18 ~.,
                        data = claims_training) %>%
  step_normalize(totpop_2018) %>%
  step_string2factor(region) %>%
  prep()

#create 10-fold
claims_resamples <- vfold_cv(data = claims_training, v = 10)
```

```{r create-custom-function-for-resamples, echo=T, message=F, warning=F}

#create custom function
train_claims <- function(split, try, tree) {
  
  #create analysis data from split
  claims_analysis <- analysis(split)
  
  #set model
  model_claims <- rand_forest(mode = "regression",
                              mtry = try,
                              trees = tree) %>%
    set_engine("ranger",
               importance = "impurity") %>%
    fit(formula = initial_claims_2020_04_18 ~ totpop_2018 + gov_republican + region,
        data = bake(object = recipe_claims,
                    new_data = claims_analysis))
  
  #create assessment data from split
  claims_assessment <- assessment(split)
  
  #get predictions on assessment data and colculate out-of-sample rmse
  rmse <- bind_cols(
    claims_assessment,
    predict(model_claims, bake(object = recipe_claims,
                               new_data = claims_assessment))) %>%
    rmse(truth = initial_claims_2020_04_18, estimate = .pred) %>%
    pull(.estimate)
  
  return(rmse)
}
```

```{r pick-model-and-train-on-training-data, echo=T, message=F, warning=F}

#calculate RMSEs: apply custom function to resamples

claims_resamples <- claims_resamples %>%
  mutate(
    rmse_1 = map_dbl(.x = splits,
                     .f = ~train_claims(split = .x,
                                        try = 2,
                                        tree = 50))
  ) %>%
  mutate(
    rmse_2 = map_dbl(.x = splits,
                     .f = ~train_claims(split = .x,
                                        try = 2,
                                        tree = 100))
  ) %>%
  mutate(
    rmse_3 = map_dbl(.x = splits,
                     .f = ~train_claims(split = .x,
                                        try = 3,
                                        tree = 500))
  )
glimpse(claims_resamples) #I got the RMSEs!

#get avg RMSE for each k
knitr::kable(rmse_avg <- c(mean(claims_resamples$rmse_1),
                           mean(claims_resamples$rmse_2),
                           mean(claims_resamples$rmse_3)),
             col.names = "RMSE"
)
```

```{r}

model_diamonds_full <-
  nearest_neighbor(mode = "regression",
                   neighbors = 100) %>%
  set_engine("kknn") %>%
  fit(formula = price ~.,
      data = bake(object = recipe_diamonds,
                  new_data = diamonds_training))

#make predictions
bind_cols(diamonds_training,
          predict(model_diamonds_full,
                  bake(object = recipe_diamonds,
                       new_data = diamonds_training))
) %>%
  rmse(truth = price, estimate = .pred)
```
