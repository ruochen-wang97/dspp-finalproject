---
title: "Predicting the Number of Unemployment Insurance Claims in the Time of COVID-19"
author: "submitted by Radhika Kaul, Odiche Nwabuikwu, & Ruochen Wang"
output:
  html_document:
    df_print: paged
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

## Question of Interest

We will develop different models to predict the number of unemployment claims filed in each US state in the second week of May (when the project is due). Our primary data source for the outcome variable is the [Unemployment Insurance Weekly Claims data](https://oui.doleta.gov/unemploy/claims.asp) from the U.S. Department of Labor (DOL)'s Employment & Training Administration.

## Project Progress

We have decided on the following variables for our models:

**Outcome Variables**: either New Initial Claims filed per week OR percentage of the labor force filing for unemployment claims per week. New Initial Claims is one of the most sensitive and most frequently used official statistics in analyzing unemployment trends (see [Aaron Soujourner and Paul Goldsmith Pinkham](https://paulgp.github.io/GoogleTrendsUINowcast/google_trends_UI.html)).

**Predictors** *(all variables are at the state level)*

Structural predictors include:

- Region
- Population
- Working Population
- Per Capita Personal Income
- Percent Population w/ a High School Degree
- Top Industry in terms of GDP Contribution
- Democratic or Republican Governor

We are considering standardizing/normalizing/logging the population variables due to concerns of high correlations between these variables and the outcome variable.

Real-Time Predictors include:

- Number of COVID Cases
- Number of Days Since the Issurance of Stay-at-Home Order
- Number of Days Before Stay-at-Home Order Ends
- Peak of Claims Predicted *(we haven't found a data source yet but will keep looking)*
- Whether there has been a protest/protests in the state

**Predictive Models**: We will have three predictive models based off of data from when the first case was reported in the US, the first death was reported in the US, and the modified date when the first death related to COVID-19 was reported (see https://www.cnn.com/2020/04/22/us/california-deaths-earliest-in-us/index.html)

**Algorithms**: KNN/CART/Random Forest. We will test each algorithm and get the error rates and pick the best one.

We propose a **Random Forest** model given that we have both categorical and continuous inputs and that we have correlated predictors. Also given the fluctuation in the weekly claims data, Random Forest will make more accurate out-of-sample predictions as compared to KNN and CART.

**Error Metrics**: RMSE (OOB Error for Random Forest)

## Data Cleaning

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, message=F, warning=F, results='hide')
```

```{r load-packages}

library(readxl)
library(haven)
library(tidyverse)
library(tidyr)
library(ggplot2)
library(treemapify)
library(ggridges)
```

### Unemployment Claims

```{r unemp-clean}

claims <- read_excel("data/raw/unemployment_bystate_weekly.xls")

claims <- claims %>%
  rename(state_name = State) %>%
  rename_all(~str_to_lower(colnames(claims))) %>%
  rename_all(~str_replace_all(., "\\s+", "_")) %>%
  filter(state != "Puerto Rico" &
           state != "Virgin Islands" ) %>%
  select(state, filed_week_ended, initial_claims, continued_claims)

# change data to wide
claims <-
  pivot_wider(data = claims,
              names_from = filed_week_ended,
              values_from = c(initial_claims, continued_claims))
```

### GDP by Industry

```{r gdp-clean}

gdp <-
  read.csv("data/raw/gdp_bystate_byindustry_2019.csv")

gdp <- gdp %>%
  filter(str_detect(string = Description,
                    pattern = "    ")) %>% 
  filter(!str_detect(string = Description, 
                     pattern = "     ")) %>% # remove aggregate & sub-industries
  filter(GeoName != "United States") %>% # remove federal data
  filter(X2019 != "(NA)" & X2019 != "(D)") %>% # remove non-numeric values
  rename(gdp2019 = X2019, state = GeoName) # rename GDP and state variables

# change data to wide
gdp <- gdp %>% 
  pivot_wider(names_from = Description, state,
              values_from = gdp2019) %>%
  filter(!state %in% c("New England", "Mideast", "Great Lakes", "Plains", "Southeast", "Southwest", "Rocky Mountain", "Far West"))

# find top industry wrt gdp
gdp <- gdp %>%
  mutate(topind_gdp = colnames(gdp)[apply(gdp, 1, which.max)]) %>%
  select(state, topind_gdp)
```

### Per Capita Personal Income

```{r inc-clean}

income <- read_csv("data/raw/percapitapersonalincome_bystate_2019.csv")

income <- income %>%
  filter(as.numeric(GeoFips) >= 1000 & as.numeric(GeoFips) <= 56000) %>% # remove fed/regional data
  rename(personal_inc2019 = "2019", state = GeoName) %>%
  select(-GeoFips)
```

### Employment by Occupation

```{r occ-clean}

occ <- read_excel("data/raw/employment_byoccupation_bystate_may2019.xlsx")

occ <- occ %>%
  filter(o_group == "major") %>%
  select(area_title, occ_code, occ_title, jobs_1000) %>%
  mutate(jobs_1000 = as.numeric(jobs_1000)) %>%
  rename(state = area_title) %>%
  group_by(state) %>%
  mutate(rank = rank(-jobs_1000)) %>%
  filter(rank <= 3) %>%
  select(-jobs_1000)

# change data to wide and rename vars
occ <- occ %>% 
  pivot_wider(names_from = rank, state,
              values_from = occ_title) %>%
  rename(topindemp_3 = 2, topindemp_1 = 3, topindemp_2 = 4)
```

ATT: need further discussion since not much variance

### Census Region

```{r regions-clean}

regions <- read_excel("data/raw/censusregion_bystate.xlsx")

regions <- regions %>%
  select(State, Region) %>%
  rename(state = State, region = Region)
```

### Merge Datasets

```{r merge}
clean_data <-
  left_join(claims, income, by = "state")
anti_join(clean_data, income)

clean_data <-
  left_join(clean_data, gdp, by = "state")
anti_join(clean_data, gdp)

clean_data <-
  left_join(clean_data, occ, by = "state")
anti_join(clean_data, occ)

clean_data <-
  left_join(clean_data, regions, by = "state")
anti_join(clean_data, regions)
```

```{r merge-sah-date}

# load & add stay at home date data
sah <- read_excel("data/raw/stayathome.xlsx")

clean_data <-
  left_join(clean_data, sah, by = "state")
anti_join(clean_data, sah)
```

```{r merge-peak-date}

#load & add peak death data

sah_peakdeaths <- read_excel("data/raw/sah_peakdeaths.xlsx")

clean_data <-
  left_join(clean_data, sah_peakdeaths, by = "state") %>%
  rename(massgath_res_date = mg_restr,
         noness_res_date = noness_restr)
anti_join(clean_data, sah_peakdeaths)
```

```{r merge-ed-attainment}

# load & add ed attainment data
edattain <- read_csv("data/raw/edattain_bystate_2018.csv")

clean_data <-
  left_join(clean_data, edattain, by = "state")
anti_join(clean_data, edattain)
```

```{r merge-working-pop}

# load & add working pop data
pctworkpop <- read_csv("data/raw/popest_bystate_2018.csv") %>%
  select(state, totpop_2018, totpop_work, pctpop_work)

clean_data <-
  left_join(clean_data, pctworkpop, by = "state")
anti_join(clean_data, pctworkpop)
```

```{r merge-gov-party,  echo=T, message=F, warning=F}

# load & add governor party data
party <- read_excel("data/raw/party_bystate_2020.xlsx")

clean_data <-
  left_join(clean_data, party, by = "state")
anti_join(clean_data, party)
```

```{r merge-death-num-till-apr10}

# load packages
library(httr)
library(RCurl)

# read in JHU COVID data from Github for April 18, 2020
jhufile_1 = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/04-18-2020.csv"

mydata <- read.csv(url(jhufile_1))

jhu_data <- mydata %>%
  rename(state = "Province_State") %>%
  drop_na(FIPS) %>%
  filter(Country_Region == "US") 

jhu_data <- jhu_data[!(is.na(jhu_data$Admin2) | jhu_data$Admin2==""),] %>%
  select(state, Last_Update, Confirmed, Deaths, Recovered)

jhu_data_final <- jhu_data %>% 
  group_by(state) %>%
  summarise(sum_deaths = sum(Deaths),
            sum_conf = sum(Confirmed))

clean_data <-
  left_join(clean_data, jhu_data_final, by = "state")
anti_join(clean_data, jhu_data_final)
```

```{r merge-incidence-and-mortality-rate-apr12}

# read in JHU data on incident and mortality rates from Github for April 18, 2020
jhufile_2 = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/04-18-2020.csv"

rates_data <- read.csv(url(jhufile_2))

jhu_data_1 <- rates_data %>%
  filter(Country_Region == "US") %>%
  rename(state = "Province_State") %>%
  select(FIPS, state, Confirmed, Deaths, Lat, Long_, Incident_Rate, Mortality_Rate) %>%
  filter(FIPS < 60)

clean_data <-
  left_join(clean_data, jhu_data_1, by = "state")
anti_join(clean_data, jhu_data_1)
```

### Create Final Analytical Data

```{r final-merge}

# remove white space from variable names
names(clean_data) <- str_replace(names(clean_data), "-", "_") 
names(clean_data) <- str_replace(names(clean_data), "-", "_") 


clean_data %>% write_csv("data/clean/merged.csv")
```

## Descriptive Analysis

```{r make-line-graph}

# build line graph data
line_data <- clean_data %>%
  select(state, 'initial_claims_2020_01_25':'initial_claims_2020_04_18', Incident_Rate) %>%
  mutate(rank = rank(-Incident_Rate)) %>%
  filter(rank <=5)

names(line_data) <- str_replace(names(line_data), "_", "-") 
names(line_data) <- str_replace(names(line_data), "_", "-") 
names(line_data) <- str_replace(names(line_data), "_", "-") 
names(line_data) <- str_replace(names(line_data), "_", "-") 

line_data_date <- line_data %>%
  gather(key = week_ended, value = initialclaims, 2:14) %>%
  mutate(week_ended = sub("initial-claims-", "", week_ended)) %>%
  mutate(week_ended = as.Date(week_ended))

# make line graph
state_rank <- c("New York",
                "New Jersey",
                "Massachusetts",
                "Louisiana",
                "Connecticut")
line_data$state <- factor(line_data$state, state_rank)

ggplot(data = line_data_date,
       mapping = aes(x = week_ended,
                     y = initialclaims,
                     group = state,
                     color = state)) +
  scale_x_date(date_breaks = "1 week") +
  geom_line() +
  geom_vline(xintercept = as.Date("2020-03-19"), linetype = 2) +
  scale_y_continuous(labels = c("0", "100,000", "200,000", "300,000", "400,000")) +
  theme_classic() +
  labs(title = "Weekly Unemployment Insurance Claims in Top 5 States\nwith Highest COVID-19 Incidence Rate",
       caption = "Source: US Department of Labor & Johns Hopkins University",
       x = "Week Ending",
       y = "Number of Initial Claims") +
  theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 4),
        plot.caption = element_text(size = 7, family = "Arial", face = "italic", vjust = -4),
        axis.title.x = element_text(family = "Arial", vjust = -2),
        axis.title.y = element_text(family = "Arial", vjust = .5),
        axis.text.x = element_text(angle = 45, size = 7, family = "Arial", hjust = 1),
        axis.text.y = element_text(family = "Arial"),
        axis.ticks.x = element_blank())
```

The graph above depicts the weekly initial unemployment claims in top five US states with the highest COVID-19 incidence rates (Connecticut, Louisiana, Massachusetts, New Jersey, and New York). The claims appear relatively low (< 50,000) until early March many states issued stay at home orders and gathering restrictions to combat the spread of COVID-19. Since many people were unable to work remotely, they were forced to file for unemployment insurance leading to a sharp increase in the number of initial unemployment claims. Due to the strong effect of these quarantine measures, we believe stay at home orders will be a good predictor for unemployment claims.


```{r make-treemap}

# build treemap data
treemap_data <- clean_data %>%
  select(state, region, `initial_claims_2020_04_18`, `continued_claims_2020_04_18`, Incident_Rate, totpop_work) %>%
  mutate(`totclaims_2020_04_18` = `continued_claims_2020_04_18` + `initial_claims_2020_04_18`) %>%
  mutate(perc_totclaims = (`totclaims_2020_04_18`/totpop_work) * 100) %>%
  select(state, region, `totclaims_2020_04_18`, Incident_Rate, perc_totclaims) %>%
  write_csv("treemap.csv")

# make treemap
ggplot(data = treemap_data,
       mapping = aes(area = `totclaims_2020_04_18`,
                     fill = Incident_Rate,
                     label = `state`,
                     subgroup = region)) +
  geom_treemap() +
  geom_treemap_subgroup_border(color = "white") +
  geom_treemap_text(color = "white",
                    place = "center",
                    grow = F,
                    reflow = T) +
  geom_treemap_subgroup_text(color = "#FAFAFA",
                             place = "center",
                             grow = T,
                             alpha = .5,
                             min.size = 0) +
  scale_fill_gradient(low = "#fdbb84", high = "#b30000",
                      breaks = seq(50, 1500, by = 500),
                      name = "Number of COVID Cases\nper 100,000 Residents") +
  labs(title = "Total Unemployment Insurance Claims Filed, as of April 18",
       caption = "Source: US Department of Labor & Johns Hopkins University") +
  theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 2),
        plot.caption = element_text(size = 7, family = "Arial", face = "italic", vjust = -2),
        legend.title = element_text(size = 9, family = "Arial", vjust = 4),
        legend.text = element_text(family = "Arial"))
```

The tree map above captures the total number of US uninsurance claims filed as of April 18th and the number of COVID-19 cases. States are grouped by region and the area each state covers is proportional to the number of claims filed. The color of each area corresponds to the number of COVID-19 cases; states with higher COVID-19 cases have darker colored areas. This visualization allows us to see regional patterns in the spread of COVID-19 as well as the number of unemployment claims filed. Most affected population have been the self-employed and gig workers, as well as workers in hospitality industry such as restaurants, who were made eligible for unemployement benefits when they experienced job losses.

```{r scatter-plot}
clean_data %>%
  mutate(perc_claim = (`continued_claims_2020_04_18`/totpop_2018) * 100) %>%
  ggplot() +
  geom_point(mapping = aes(x = perc_claim, y = Incident_Rate, 
                           color = as.character(gov_republican)),
             alpha = 0.5) + 
  scale_color_manual(name = "Party of State Governor",
                       labels = c("Democrat", "Republican"), 
                     values = c("blue", "red")) +
  labs(title = 
         paste("COVID-19 Incidence and Continued Claims by State Party"),
       caption = "Source: US Department of Labor & Johns Hopkins University",
       x = "Percent of work force that filed claims",
       y = "Incidence Rate") + 
  theme_minimal() +
  theme(panel.grid.major = element_line(linetype = "dotted")) +
  theme(panel.grid.minor = element_blank()) +
   theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 2),
        plot.caption = element_text(size = 7, family = "Arial", face = "italic", vjust = -2),
        legend.title = element_text(size = 9, family = "Arial", vjust = 4),
        legend.text = element_text(family = "Arial")) 


```

The scatterplot above shows the relationship between continued unemployment claims filed and the COVID-19 incidence rate among US states. The points on the scatterplot are also colored by the party of the state governor. The graph shows a moderately positive relationship between COVID-19 incidence rates and continued claims as a percentage of the working population. This makes sense as the spread of COVID-19 has forced employers to shut down, leaving many unemployed. The relationship appears to be mostly driven by states with Democrat governors, as many of them have higher incidence rates than Republican states. This relationship is informative for our model becasue it shows that COVID-19 incidence rates are associated (to some degree) with the number of unisurance claims, which makes it a good predictor. 


```{r density-plot}
# create variable for peak since April 18th

clean_data <- clean_data %>%
  mutate(peak = days_since_peak_deaths - 14) 

# this plot shows the distribution of days since peak deaths (as of april 18th) divided by region

ggplot(data = clean_data, mapping = aes(x = peak, y = as.factor(region))) +
  geom_density_ridges(fill = "#1696d2") +
  labs(title = 
         paste("Distribution of Days Since Predicted COVID-19 \nPeak Deaths by Region"),
       caption = "Source: Institute for Health Metrics and Evaluation",
       x = "Days since peak deaths (as of April 18th)",
       y = "Region") +
  theme_minimal() +
   theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 2),
        plot.caption = element_text(size = 7, family = "Arial", face = "italic", vjust = -2),
        legend.title = element_text(size = 9, family = "Arial", vjust = 4),
        legend.text = element_text(family = "Arial")) 
  
```

The ridgeplot above shows the distribution of the days since peak COVID-19 deaths broken down by region. Days since peak deaths is a predicted variable generated by the IHME. Although this visualization only shows braod patterns, it is useful for understanding which regions will combat COVID-19 for a longer period. This is informative because we can expect these regions to maintain quarantine measures for longer, which may result in increased unemployment claims in the coming weeks. Thus this variable would be a useful predictor for our model. 



