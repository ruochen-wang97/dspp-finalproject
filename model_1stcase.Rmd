---
title: "Candidate Model 1 (BEST MODEL)"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, message=F, warning=F)
```

```{r load-packages, echo=T, message=F, warning=F}

library(haven)
library(readxl)
library(tidyverse)
library(lubridate)
library(rsample)
library(recipes)
library(parsnip)
library(yardstick)
library(ranger)
library(ggalt)
library(forcats)
```

```{r reshape-data, echo=T, message=F, warning=F}

# read in data
claims_data <- read_csv("data/clean/merged.csv")
date_var <- read_excel("abc.xlsx")

# reshape data to long (kind of)
initial_long <- claims_data %>%
  pivot_longer(cols = 
                 initial_claims_2020_01_04:initial_claims_2020_04_18, 
               names_to = "initial_type", values_to = "initial_num") %>%
  left_join(date_var)

continued_long <- claims_data %>%
  pivot_longer(cols = 
                 continued_claims_2020_01_04:continued_claims_2020_04_18,
               names_to = "continued_type", values_to = "continued_num") %>%
  left_join(date_var) %>%
  select(state, date, continued_type, continued_num)

claims_long <-
  left_join(initial_long, continued_long, by = c("state","date"))
anti_join(initial_long, continued_long)

# select variables
claims_long <- claims_long %>%
  select(initial_type, initial_num, continued_type.x, continued_num, 
         date, sah_date, days_since_peak_deaths, Incident_Rate, 
         Mortality_Rate, region, topind_gdp, topindemp_1, topindemp_2, 
         topindemp_3, totpop_2018, pctpop_work, pctpop_hs, 
         personal_inc2019, gov_republican, state) %>%
  mutate(days_since_sah = as.Date(date) - ymd(sah_date)) %>%
  mutate(days_since_sah = replace_na(days_since_sah, 0)) %>%
  mutate(days_since_peak =
           days_since_peak_deaths - (as.Date("2020-05-01") - as.Date(date))) %>%
  mutate(gov_republican = factor(gov_republican)) %>%
  filter(as.Date(date) >= as.Date("2020-01-25")) %>% #filter to 1st case
  select(-sah_date, -days_since_peak_deaths)
```

```{r split-and-preprocess-data-for-rf, echo=T, message=F, warning=F}

set.seed(20200505)

# split data into training and test sets
claims_training <- claims_long %>%
  filter(date < ymd("2020-04-18"))
claims_testing <- claims_long %>%
  filter(date >= ymd("2020-04-18"))

# create recipe for rf
recipe_claims <- recipe(formula = initial_num ~ continued_num + 
                          days_since_sah + days_since_peak + 
                          Incident_Rate + Mortality_Rate + 
                          region + topind_gdp + topindemp_1 + topindemp_2
                        + topindemp_3 + totpop_2018 + pctpop_work + 
                          pctpop_hs + personal_inc2019 + gov_republican,
                        data = claims_training) %>%
  step_string2factor(region, topind_gdp, topindemp_1, topindemp_2, topindemp_3) %>%
  prep()

# create 10-fold
claims_resamples <- bootstraps(data = claims_training, times = 10)
```

```{r create-custom-function-for-resamples, echo=T, message=F, warning=F}

# create custom function
train_claims <- function(model, split, try, tree) {
  
  # create analysis data from split
  claims_analysis <- analysis(split)
  
  # set models - Random Forest and CART
  if(model == "rf") {
    model_claims <- rand_forest(mode = "regression",
                                mtry = try,
                                trees = tree) %>%
      set_engine("ranger",
                 importance = "impurity") %>%
      fit(formula = initial_num ~ continued_num + days_since_sah + 
            days_since_peak + Incident_Rate + Mortality_Rate + region + 
            topind_gdp + topindemp_1 + topindemp_2 + topindemp_3 + 
            totpop_2018 + pctpop_work + pctpop_hs + personal_inc2019 + 
            gov_republican,
          data = bake(object = recipe_claims,
                      new_data = claims_analysis))
  }
  else if(model == "cart") {
    model_claims <- decision_tree(mode = "regression") %>%
      set_engine("rpart") %>%
      fit(formula = initial_num ~ continued_num + days_since_sah + 
            days_since_peak + Incident_Rate + Mortality_Rate + region + 
            topind_gdp + topindemp_1 + topindemp_2 + topindemp_3 + 
            totpop_2018 + pctpop_work + pctpop_hs + personal_inc2019 + 
            gov_republican,
          data = bake(object = recipe_claims,
                      new_data = claims_analysis))
  }
  
  # create assessment data from split
  claims_assessment <- assessment(split)
  
  # get predictions on assessment data and rmse for each split
  rmse <- bind_cols(
    claims_assessment,
    predict(model_claims, bake(object = recipe_claims,
                               new_data = claims_assessment))) %>%
    rmse(truth = initial_num, estimate = .pred) %>%
    pull(.estimate)
  
  return(rmse)
}
```

```{r pick-model, echo=T, message=F, warning=F}

# apply custom function to resamples
claims_resamples <- claims_resamples %>%
  mutate(
    rmse_rf_1 = map_dbl(.x = splits,
                        .f = ~train_claims(model = "rf",
                                           split = .x,
                                           try = 5,
                                           tree = 100))
  ) %>%
  mutate(
    rmse_rf_2 = map_dbl(.x = splits,
                        .f = ~train_claims(model = "rf",
                                           split = .x,
                                           try = 10,
                                           tree = 500))
  ) %>%
  mutate(
    rmse_rf_3 = map_dbl(.x = splits,
                        .f = ~train_claims(model = "rf",
                                           split = .x,
                                           try = 5,
                                           tree = 500))
  ) %>%
  mutate(
    rmse_cart = map_dbl(.x = splits,
                        .f = ~train_claims(model = "cart",
                                           split = .x))
  )

glimpse(claims_resamples)

# compare rf and cart
claims_resamples <- claims_resamples %>%
  select(-splits)

knitr::kable(claims_resamples)
knitr::kable(rmse_avg <- c(mean(claims_resamples$rmse_rf_1),
                           mean(claims_resamples$rmse_rf_2),
                           mean(claims_resamples$rmse_rf_3),
                           mean(claims_resamples$rmse_cart)),
             col.names = "RMSE"
)
```

```{r visualize-RMSEs-for-resamples, echo=T, message=F, warning=F}

claims_resamples %>%
  pivot_longer(-id) %>%
  ggplot(aes(x = id,
             y = value,
             color = name,
             group = name)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, NA)) +
  theme_minimal()
```

```{r est-out-of-sample-rate, echo=T, message=F, warning=F}

# prediction on testing data
model_final <-
  rand_forest(mode = "regression",
              mtry = 5,
              trees = 500) %>%
  set_engine("ranger",
             importance = "impurity") %>%
  fit(initial_num ~
        continued_num +
        days_since_sah + days_since_peak + Incident_Rate + Mortality_Rate
      + region + topind_gdp + topindemp_1 + topindemp_2 + topindemp_3 + 
        totpop_2018 + pctpop_work + pctpop_hs + personal_inc2019 +
        gov_republican,
      data = bake(object = recipe_claims,
                  new_data = claims_training))

# add predicted values column to testing data
claims_testing <- bind_cols(
  claims_testing,
  pred_claims_2020_04_18 = predict(model_final, claims_testing)
) 

# add predicted values column to training data
claims_training <- bind_cols(
  claims_training,
  pred_claims = predict(model_final, claims_training)
) 

# Error metric estimation
bind_cols(
  claims_testing,
  pred_claims_2020_04_18 = predict(model_final, claims_testing)
) %>%
  metrics(truth = initial_num, 
          estimate = .pred)
```


### Residual Plots

```{r plot-dumbbell-testing}

# create residual column for training and testing data
claims_training <- claims_training %>%
  mutate(residual = .pred - initial_num)

claims_testing <- claims_testing %>%
  mutate(residual = .pred - initial_num)

#creating a factor vector of the STATE names to 
claims_testing$state <- factor(claims_testing$state)
#reverse the order of the STATEs
library(forcats)
claims_testing$state_order <- fct_rev(claims_testing$state)

# dumbbell plot for showing actual and predicted values for testing data
# edit testing data for plot
claims_reshape <- claims_testing %>%
  select(initial_num, .pred, state) %>%
  rename(Actual = initial_num, Predicted = .pred) %>%
  pivot_longer(cols = c("Actual", "Predicted"), 
                names_to = "value_type", 
               values_to =  "value") %>%
  mutate(state = fct_rev(factor(state)))

claims_testing <- claims_testing %>%
  mutate(state = fct_rev(factor(state)))

ggplot() + 
<<<<<<< HEAD
  geom_dumbbell(data = claims_testing, mapping = aes(x = initial_num, xend = .pred, y = state),
                size=1.5, color="gray74", 
                colour_x = "blue", colour_xend = "orange",
                dot_guide=FALSE, show.legend = TRUE) +
  geom_point(data = claims_reshape, 
             mapping = aes(x = value, y = state, color = value_type), size = 1.5) +
  scale_color_manual(name = "Initial Claims", values = c("blue", "orange"),
                     labels = c("Observed", "Predicted")) +
  theme_minimal() +
  scale_x_continuous(breaks = c(0, 200000, 400000, 600000), 
                     labels = c("0", "200000", "400000", "600000")) +
  labs(title = "Differences Between Observed and \nPredicted Values in Testing Data",
       x = "Initial Claims (April 18)",
       y = "State") +
  theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 4),
        axis.title.x = element_text(family = "Arial", vjust = -2),
        axis.title.y = element_text(family = "Arial", vjust = .5),
        axis.text.x = element_text(family = "Arial", hjust = 1),
        axis.text.y = element_text(family = "Arial"),
        axis.ticks.x = element_blank()) +
  theme(legend.position = "bottom")  +
  theme(panel.grid.major.x  = element_blank()) +
  theme(panel.grid.minor.x = element_blank()) 
=======
  geom_dumbbell(data = claims_testing, 
                mapping = aes(x = initial_num, xend = .pred, y = state_order),
                size=1.3, color="#e3e2e1", 
                colour_x = "blue", colour_xend = "orange",
                dot_guide=FALSE, show.legend = TRUE) +
  #geom_point(data = claims_testing, aes(x = initial_num, 
                                        #color = group), size = 3)
    labs(title = "Difference between actual claims and claimed predicted by ML algorithm",
       x = "Initial Claims (April 18,2020)",
       y = "State")+
  theme_bw()
>>>>>>> 40a3318e050f1504fde92b04c5adf4518e5c48b3

```

The dumbbell plot above shows the state differences between the actual and predicted intitial unmeployment claims for April 18th. This visualization is a good way to compare residuals among states. For example, the plot shows that our predictions were most accurate for West Virginia, Kentucky and Alaska while they were least accurate for Michigan New York and California.

```{r scatter-incidence-residuals}

ggplot(data = claims_testing) +
  geom_point(aes(x = Incident_Rate, y = residual), shape = 1, color = "mediumblue") +
  geom_hline(yintercept = 0, linetype = "solid", size = 0.5) +
  theme_minimal() +
scale_y_continuous(breaks = c(-300000, -200000, -100000,  0, 100000, 200000), 
                     labels = c("-300000", "-200000", "-100000", "0", "100000", "200000")) +
  labs(title = "RF Residuals and COVID-19 Incidence Rates",
       x = "COVID-19 Incidence Rates",
       y = "Residuals") +
  theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 4),
        axis.title.x = element_text(family = "Arial", vjust = -2),
        axis.title.y = element_text(family = "Arial", vjust = .5),
        axis.text.x = element_text(family = "Arial", hjust = 1),
        axis.text.y = element_text(family = "Arial"),
        axis.ticks.x = element_blank()) +
  theme(panel.grid.major = element_blank()) 

```

The scatterplot above shows the relationship between the model residuals and COVID-19 incidence rates. The points in the plot generally show a positive relationship between the model residuals and COVID-19 incidence rates (with the exception of Florida). This means that our predictions were generally more accurate for states with a lower incidence of COVID-19. 

<br>
<br>


We also construct plots to compare our residuals with observed initial claims. We construct these plots for predictions in the testing and training data. 

```{r res-plot-training-training}

### Residual Plots for Training Data

ggplot(data = claims_training) +
  geom_point(aes(x = initial_num, y = residual), shape = 1, color = "mediumblue") +
  geom_hline(yintercept = 0, linetype = "solid", size = 0.5) +
  theme_minimal() +
  scale_y_continuous(breaks = c(-400000, -200000, 0, 200000), 
                     labels = c("-400000", "-200000", "0", "200000")) +
  labs(title = "Residuals for RF Training Data",
       x = "Observed Initial Claims (Jan 25 - April 11)",
       y = "Residuals") +
  theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 4),
        axis.title.x = element_text(family = "Arial", vjust = -2),
        axis.title.y = element_text(family = "Arial", vjust = .5),
        axis.text.x = element_text(family = "Arial", hjust = 1),
        axis.text.y = element_text(family = "Arial"),
        axis.ticks.x = element_blank()) +
  theme(panel.grid.major = element_blank()) 

```
The plot above compares observed values of initial claims to our model's residuals in the training data. Residuals appear generally low and clustered around zero. Residuals also appear to be evenly split between positive and negative values, so our model does not appear to be over or underestimating predictions.



```{r res-plot-training-testing}

### Residual Plots for Testing Data

ggplot(data = claims_testing) +
  geom_point(aes(x = initial_num, y = residual), shape = 1, color = "mediumblue") +
  geom_hline(yintercept = 0, linetype = "solid", size = 0.5) +
  theme_minimal() +
  scale_y_continuous(breaks = c(-300000, -200000, -100000,  0, 100000, 200000), 
                     labels = c("-300000", "-200000", "-100000", "0", "100000", "200000")) +
  scale_x_continuous(breaks = c(0, 200000, 400000), 
                     labels = c("0", "200000", "400000")) +
  labs(title = "Residuals for RF Testing Data",
       x = "Observed Initial Claims (April 18)",
       y = "Residuals") +
  theme(plot.margin = unit(c(.5, .5, .5, .5),"cm"),
        plot.title = element_text(size = 16, family = "Arial", face = "bold", vjust = 4),
        axis.title.x = element_text(family = "Arial", vjust = -2),
        axis.title.y = element_text(family = "Arial", vjust = .5),
        axis.text.x = element_text(family = "Arial", hjust = 1),
        axis.text.y = element_text(family = "Arial"),
        axis.ticks.x = element_blank()) +
  theme(panel.grid.major = element_blank()) 

```

The plot above compares observed values of initial claims to our model's residuals in the testing data. Here, residuals are more spread out than in the training data, indicating a lot of variation in the distribution of predicted values. Most residual are also above zero, indicating that our predictions were generally overestimating initial claims. The variation and positive bias of residuals may indicate that we may have overfitted our model for the training data, which made it perform poorly in the testing data. 


